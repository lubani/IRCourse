# -*- coding: utf-8 -*-
from datetime import time, datetime
import time
import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import sklearn
from matplotlib import pyplot as plt
# calculate accuracy of class predictions
from sklearn import metrics
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from imblearn.over_sampling import SMOTE
from sklearn.utils import shuffle
from mlxtend.plotting import plot_confusion_matrix
from imblearn.under_sampling import TomekLinks
from sklearn.utils.class_weight import compute_class_weight

start_time = time.time()
# Create the vectorizer
vectorizer = TfidfVectorizer(norm='l2')


def manual_separation(bad_line):
    right_split = bad_line[:-2] + [",".join(
        bad_line[-2:])]  # All the "bad lines" where all coming from the same last column that was containing ","
    return right_split


# Read the revievs
df = pd.read_csv('Reviews.csv', engine="python")

# ------ Show the data --------
# df
# df.info()
# df.describe()
# ------ Show the data --------

df['Helpful%'] = np.where(df['HelpfulnessDenominator'] > 0,
                          df['HelpfulnessNumerator'] / df['HelpfulnessDenominator'],
                          -1)
df['upvote%'] = pd.cut(df['Helpful%'], bins=[-1, 0, 0.2, 0.4, 0.6, 0.8, 1],
                       labels=['Empty', '0-20%', '20-40%', '40-60%', '60-80%', '80-100%'])
print(
    df.groupby(['Score', 'upvote%']).agg({'Id': 'count'}))  # Bin labels must be one fewer
# than the number of bin edges
df_s = df.groupby(['Score', 'upvote%']).agg({'Id': 'count'}).reset_index()
pivot = df_s.pivot(index='upvote%', columns='Score')
sns.heatmap(pivot, annot=True, cmap='YlGnBu', fmt='g')

# Ignoring datapoints with score=3

df2 = df[df['Score'] != 3]
# Data Prepare
X = df2['Text']
y_dict = {1: 0, 2: 0, 4: 1, 5: 1}
y = df2['Score'].map(y_dict)
X, y = shuffle(X, y)
X = X.head(100000)
y = y.head(100000)
labels = np.unique(y)
# Set the number of folds
n_folds = 5
# Create the k-fold cross-validator
kf = KFold(n_splits=n_folds)


# - Bag of Words
# c = CountVectorizer(stop_words='english')                  # to ignore all english stopwords
# x_c = c.fit_transform(x)

# get train test
# x_train,x_test,y_train,y_test = train_test_split(x,y)


def predictNB(clf, X_test):
    # Create a numerical representation of the data to make predictions on using the vectorizer
    # X_set = vectorizer.transform(strings)
    # X_test = X_test.toarray()

    # Make predictions using the classifier
    return clf.predict(X_test)


def trainNB(X_train, y_train):
    # Create an empty list to store the strings generated by the X_train iterable
    # strings = []
    #
    # # Iterate over the elements of the X_train iterable and append each string to the strings list
    # for i, stringi in enumerate(X_train):
    #     # Add a new item to the strings dictionary using the index of the item
    #     # in the y_train series as the key
    #     strings.append(stringi)

    # Create a numerical representation of the data to train the classifier on using the vectorizer
    # X_set = vectorizer.fit_transform(strings)
    # X_train = X_train.toarray()

    # Train the classifier

    # Compute class weights based on the frequency of each class in the training data
    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)

    # Create a sample weight array using the computed class weights
    sample_weight = np.array([class_weights[c] for c in y_train])

    # Fit the model using the sample weight array
    nb = MultinomialNB()
    nb.fit(X_train, y_train, sample_weight=sample_weight)

    return nb


def generate_augmented_stringNB(string, label):
    # Generate a list of random characters to add to the string
    global augmented_string
    added_characters = np.random.choice(list(string.lower()), size=5)

    # Add the characters to the string at random positions
    for character in added_characters:
        # Generate a random position to insert the character
        position = np.random.randint(0, len(string))
        # Insert the character at the specified position
        augmented_string = string[:position] + character + string[position:]

    return augmented_string, label


def make_df(cols, ind):
    """Quickly make a DataFrame"""
    data = {c: [str(c) + str(i) for i in ind]
            for c in cols}
    return pd.DataFrame(data, ind)


df_sample = pd.concat([X, y], axis=1)

print(df_sample)
augmented_data = pd.DataFrame(columns=['Text', 'Score'])
# Logistic Regression on Bag of Words
# log = LogisticRegression(solver='liblinear')
# ml = log.fit(x_train, y_train)
# print(confusion_matrix(y_test, log.predict(x_test)))

# Loop over the sampled data and generate new strings for each row
i = 0
for _, row in df_sample.iterrows():
    # Get the original string and label
    string_ = row['Text']
    label = row['Score']

    # Generate a new string based on the original string
    augmented_string = generate_augmented_stringNB(string_, label)

    # Add the new string and label to the augmented data DataFrame
    augmented_data = pd.concat([augmented_data, pd.DataFrame({'Text': [augmented_string], 'Score':
        [label]})], ignore_index=True)
    i += 1
    if i % 1000 == 0:
        print(i)
    if i % 50000 == 0:
        break

# Finally, concatenate the augmented data with the original data to create a new DataFrame
augmented_df = pd.concat([df_sample, augmented_data], axis=0)
# Add the new data to the dataframe
# df = pd.concat([df, new_data], axis=1)
# Training data
X = augmented_df["Text"].astype(str)
y = augmented_df["Score"].astype(int)
X = vectorizer.fit_transform(X)
# Oversample the minority class using SMOTE
# smote = SMOTE(sampling_strategy='auto')
# X, y = smote.fit_resample(X, y)
# Create the TomekLinks object
tl = TomekLinks(sampling_strategy='auto')
# Fit the TomekLinks object to the data
# X, y = tl.fit_resample(X, y)
# Shuffle the balanced dataset
X, y = shuffle(X, y)

# The following code segment is without using K-Fold.
# Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
# # Train the class
# clf = trainNB(X_train, y_train)
# predictions = predictNB(clf, X_test)
#
# # Create a DataFrame with the predictions and the actual labels
# results = pd.DataFrame({"Text": X_test, "Predicted": predictions, "Actual": y_test})
# cm = metrics.confusion_matrix(y_test, predictions)
#
# print("accuracy: {:.2f}%".format(metrics.accuracy_score(y_test, predictions) * 100))
# print("precision: {:.2f}%".format(metrics.precision_score(y_test, predictions) * 100))
#
#
# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
# disp.plot(values_format='')
# plt.show()
# results.to_csv("resultsNBKfold.csv")
#
# # Filter the DataFrame to only include rows where the prediction was incorrect
# incorrect_predictions_df = results[results['Predicted'] != y_test]
#
# # Save the DataFrame to a CSV file
# incorrect_predictions_df.to_csv('incorrect_predictions_reviews.csv', index=False)
# Create an empty dataframe to store the results
results = pd.DataFrame(columns=['y_pred', 'y_test'])

# Initialize a list to store the evaluation metrics for each fold
accuracies = []
precisions = []
cms = []

print(f'labels = {labels}')
# Split the data into folds using the KFold object
for train_index, test_index in kf.split(X):
    X_trainKF, X_testKF = X[train_index], X[test_index]
    y_trainKF, y_testKF = y.iloc[train_index], y.iloc[test_index]
    # Train the classifier using the training data
    clf = trainNB(X_trainKF, y_trainKF)

    # Make predictions using the classifier
    y_predKF = predictNB(clf, X_testKF)

    # Calculate and store the evaluation metric for the current fold
    accuracies.append(metrics.accuracy_score(y_testKF, y_predKF))
    # Compute the precision score
    precisions.append(metrics.precision_score(y_testKF, y_predKF))
    cms.append(metrics.confusion_matrix(y_testKF, y_predKF, labels=labels))
    # Create a dataframe with the predictions and actual values for the current fold
    fold_results = pd.DataFrame({'y_pred': y_predKF, 'y_test': y_testKF}).reset_index(drop=True)

    # Add the SMS text strings to the dataframe
    fold_results = fold_results.assign(sms=X_testKF.shape[0])
    # results = results.assign(sms=X_test)
    # Append the results of the current fold to the overall results dataframe
    results = pd.concat([results, fold_results], ignore_index=True)

# Calculate the mean and standard deviation of the evaluation metric across all folds
mean_accuracy = np.mean(accuracies)
std_dev = np.std(accuracies)
mean_precision = np.mean(precisions)
print("Mean accuracy: {:.2f}%".format(mean_accuracy * 100))
print("Mean precision: {:.2f}%".format(mean_precision * 100))
print("Standard deviation: {:.2f}%".format(std_dev * 100))
# Calculate the average confusion matrix
cm_avg = np.sum(cms, axis=0) / n_folds
print(cm_avg)
disp = ConfusionMatrixDisplay(confusion_matrix=(np.round(cm_avg)).astype(int), display_labels=labels)
disp.plot(values_format='')
plt.show()
# Get the current time after the program has run
end_time = time.time()

# Calculate the total runtime of the program in seconds
runtime_seconds = end_time - start_time

# Convert the runtime from seconds to a datetime object
runtime = datetime.fromtimestamp(runtime_seconds)

# Print the total runtime of the program
print("Total runtime of the program:", runtime.minute, "minutes and", runtime.second, "seconds.")
